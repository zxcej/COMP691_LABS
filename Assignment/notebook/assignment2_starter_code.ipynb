{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zxcej/COMP691_LABS/blob/main/assignment2_starter_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNtvb_zpN4H7"
      },
      "source": [
        "# ü¶ú NN-Based Language Model\n",
        "In this excercise we will run a basic RNN based language model and answer some questions about the code. It is advised to use GPU to run the code. First run the code then answer the questions below that require modifying it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CTOJyYyujICY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "c:\\Users\\x_zhu202\\Documents\\GitHub\\COMP691_LABS\\pytorch-tutorial\\tutorials\\02-intermediate\\language_model\n",
            "--> Device selected: cuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'pytorch-tutorial' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "#@title üßÆ Imports & Hyperparameter Setup\n",
        "#@markdown Feel free to experiment with the following hyperparameters at your\n",
        "#@markdown leasure. For the purpose of this assignment, leave the default values\n",
        "#@markdown and run the code with these suggested values.\n",
        "# Some part of the code was referenced from below.\n",
        "# https://github.com/pytorch/examples/tree/master/word_language_model \n",
        "# https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/language_model\n",
        "\n",
        "! git clone https://github.com/yunjey/pytorch-tutorial/\n",
        "%cd pytorch-tutorial/tutorials/02-intermediate/language_model/\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "embed_size = 128 #@param {type:\"number\"}\n",
        "hidden_size = 1024 #@param {type:\"number\"}\n",
        "num_layers = 1 #@param {type:\"number\"}\n",
        "num_epochs = 5 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "batch_size = 20 #@param {type:\"number\"}\n",
        "seq_length = 30 #@param {type:\"number\"}\n",
        "learning_rate = 0.002 #@param {type:\"number\"}\n",
        "#@markdown Number of words to be sampled ‚¨áÔ∏è\n",
        "num_samples = 50 #@param {type:\"number\"}  \n",
        "\n",
        "print(f\"--> Device selected: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tzj73P_QeBEA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vcoabulary size: 10000\n",
            "Number of batches: 1549\n"
          ]
        }
      ],
      "source": [
        "from data_utils import Dictionary, Corpus\n",
        "\n",
        "# Load \"Penn Treebank\" dataset\n",
        "corpus = Corpus()\n",
        "ids = corpus.get_data('data/train.txt', batch_size)\n",
        "vocab_size = len(corpus.dictionary)\n",
        "num_batches = ids.size(1) // seq_length\n",
        "\n",
        "print(f\"Vcoabulary size: {vocab_size}\")\n",
        "print(f\"Number of batches: {num_batches}\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MKzalmp8dndK"
      },
      "source": [
        "## ü§ñ Model Definition\n",
        "As you can see below, this model stacks `num_layers` many [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) units vertically to construct our basic RNN-based language model. The diagram below shows a pictorial representation of the model in its simplest form (i.e `num_layers`=1)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QZTjM5fQri35"
      },
      "outputs": [],
      "source": [
        "# RNN based language model\n",
        "class RNNLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super(RNNLM, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        \n",
        "    def forward(self, x, h):\n",
        "        # Embed word ids to vectors\n",
        "        x = self.embed(x)\n",
        "        \n",
        "        # Forward propagate LSTM\n",
        "        out, (h, c) = self.lstm(x, h)\n",
        "        \n",
        "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
        "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
        "        \n",
        "        # Decode hidden states of all time steps\n",
        "        out = self.linear(out)\n",
        "        return out, (h, c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_fjTZ6wdpae"
      },
      "source": [
        "## üèì Training\n",
        "In this section we will train our model, this should take a couple of minutes! Be patient üòä"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DsaIIUUHjQ5n"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Step[0/1549], Loss: 9.2069, Perplexity: 9965.54\n",
            "Epoch [1/5], Step[100/1549], Loss: 6.0190, Perplexity: 411.16\n",
            "Epoch [1/5], Step[200/1549], Loss: 5.9211, Perplexity: 372.81\n",
            "Epoch [1/5], Step[300/1549], Loss: 5.7155, Perplexity: 303.54\n",
            "Epoch [1/5], Step[400/1549], Loss: 5.6774, Perplexity: 292.19\n",
            "Epoch [1/5], Step[500/1549], Loss: 5.1143, Perplexity: 166.39\n",
            "Epoch [1/5], Step[600/1549], Loss: 5.2022, Perplexity: 181.68\n",
            "Epoch [1/5], Step[700/1549], Loss: 5.3083, Perplexity: 202.01\n",
            "Epoch [1/5], Step[800/1549], Loss: 5.2033, Perplexity: 181.86\n",
            "Epoch [1/5], Step[900/1549], Loss: 5.0703, Perplexity: 159.23\n",
            "Epoch [1/5], Step[1000/1549], Loss: 5.1053, Perplexity: 164.90\n",
            "Epoch [1/5], Step[1100/1549], Loss: 5.3635, Perplexity: 213.47\n",
            "Epoch [1/5], Step[1200/1549], Loss: 5.1588, Perplexity: 173.95\n",
            "Epoch [1/5], Step[1300/1549], Loss: 5.1094, Perplexity: 165.56\n",
            "Epoch [1/5], Step[1400/1549], Loss: 4.8076, Perplexity: 122.44\n",
            "Epoch [1/5], Step[1500/1549], Loss: 5.1488, Perplexity: 172.23\n",
            "Epoch [2/5], Step[0/1549], Loss: 5.4710, Perplexity: 237.69\n",
            "Epoch [2/5], Step[100/1549], Loss: 4.5282, Perplexity: 92.60\n",
            "Epoch [2/5], Step[200/1549], Loss: 4.7244, Perplexity: 112.66\n",
            "Epoch [2/5], Step[300/1549], Loss: 4.7133, Perplexity: 111.41\n",
            "Epoch [2/5], Step[400/1549], Loss: 4.5950, Perplexity: 98.98\n",
            "Epoch [2/5], Step[500/1549], Loss: 4.0911, Perplexity: 59.81\n",
            "Epoch [2/5], Step[600/1549], Loss: 4.4466, Perplexity: 85.33\n",
            "Epoch [2/5], Step[700/1549], Loss: 4.3328, Perplexity: 76.16\n",
            "Epoch [2/5], Step[800/1549], Loss: 4.4553, Perplexity: 86.08\n",
            "Epoch [2/5], Step[900/1549], Loss: 4.2215, Perplexity: 68.13\n",
            "Epoch [2/5], Step[1000/1549], Loss: 4.3278, Perplexity: 75.78\n",
            "Epoch [2/5], Step[1100/1549], Loss: 4.5604, Perplexity: 95.62\n",
            "Epoch [2/5], Step[1200/1549], Loss: 4.4022, Perplexity: 81.63\n",
            "Epoch [2/5], Step[1300/1549], Loss: 4.2522, Perplexity: 70.26\n",
            "Epoch [2/5], Step[1400/1549], Loss: 3.9612, Perplexity: 52.52\n",
            "Epoch [2/5], Step[1500/1549], Loss: 4.3102, Perplexity: 74.45\n",
            "Epoch [3/5], Step[0/1549], Loss: 4.4508, Perplexity: 85.69\n",
            "Epoch [3/5], Step[100/1549], Loss: 3.8243, Perplexity: 45.80\n",
            "Epoch [3/5], Step[200/1549], Loss: 4.0313, Perplexity: 56.34\n",
            "Epoch [3/5], Step[300/1549], Loss: 4.0065, Perplexity: 54.95\n",
            "Epoch [3/5], Step[400/1549], Loss: 3.8685, Perplexity: 47.87\n",
            "Epoch [3/5], Step[500/1549], Loss: 3.4581, Perplexity: 31.76\n",
            "Epoch [3/5], Step[600/1549], Loss: 3.7979, Perplexity: 44.61\n",
            "Epoch [3/5], Step[700/1549], Loss: 3.6312, Perplexity: 37.76\n",
            "Epoch [3/5], Step[800/1549], Loss: 3.7922, Perplexity: 44.35\n",
            "Epoch [3/5], Step[900/1549], Loss: 3.5325, Perplexity: 34.21\n",
            "Epoch [3/5], Step[1000/1549], Loss: 3.6896, Perplexity: 40.03\n",
            "Epoch [3/5], Step[1100/1549], Loss: 3.7728, Perplexity: 43.50\n",
            "Epoch [3/5], Step[1200/1549], Loss: 3.7255, Perplexity: 41.49\n",
            "Epoch [3/5], Step[1300/1549], Loss: 3.4818, Perplexity: 32.52\n",
            "Epoch [3/5], Step[1400/1549], Loss: 3.2329, Perplexity: 25.35\n",
            "Epoch [3/5], Step[1500/1549], Loss: 3.6102, Perplexity: 36.97\n",
            "Epoch [4/5], Step[0/1549], Loss: 3.6704, Perplexity: 39.27\n",
            "Epoch [4/5], Step[100/1549], Loss: 3.2356, Perplexity: 25.42\n",
            "Epoch [4/5], Step[200/1549], Loss: 3.4683, Perplexity: 32.08\n",
            "Epoch [4/5], Step[300/1549], Loss: 3.4890, Perplexity: 32.75\n",
            "Epoch [4/5], Step[400/1549], Loss: 3.4009, Perplexity: 29.99\n",
            "Epoch [4/5], Step[500/1549], Loss: 2.9435, Perplexity: 18.98\n",
            "Epoch [4/5], Step[600/1549], Loss: 3.3262, Perplexity: 27.83\n",
            "Epoch [4/5], Step[700/1549], Loss: 3.1705, Perplexity: 23.82\n",
            "Epoch [4/5], Step[800/1549], Loss: 3.3215, Perplexity: 27.70\n",
            "Epoch [4/5], Step[900/1549], Loss: 3.0867, Perplexity: 21.90\n",
            "Epoch [4/5], Step[1000/1549], Loss: 3.2723, Perplexity: 26.37\n",
            "Epoch [4/5], Step[1100/1549], Loss: 3.2085, Perplexity: 24.74\n",
            "Epoch [4/5], Step[1200/1549], Loss: 3.1873, Perplexity: 24.22\n",
            "Epoch [4/5], Step[1300/1549], Loss: 3.0351, Perplexity: 20.80\n",
            "Epoch [4/5], Step[1400/1549], Loss: 2.6961, Perplexity: 14.82\n",
            "Epoch [4/5], Step[1500/1549], Loss: 3.1527, Perplexity: 23.40\n",
            "Epoch [5/5], Step[0/1549], Loss: 3.2509, Perplexity: 25.81\n",
            "Epoch [5/5], Step[100/1549], Loss: 2.8687, Perplexity: 17.61\n",
            "Epoch [5/5], Step[200/1549], Loss: 3.0505, Perplexity: 21.13\n",
            "Epoch [5/5], Step[300/1549], Loss: 3.1495, Perplexity: 23.32\n",
            "Epoch [5/5], Step[400/1549], Loss: 3.0962, Perplexity: 22.11\n",
            "Epoch [5/5], Step[500/1549], Loss: 2.6367, Perplexity: 13.97\n",
            "Epoch [5/5], Step[600/1549], Loss: 2.9966, Perplexity: 20.02\n",
            "Epoch [5/5], Step[700/1549], Loss: 2.8827, Perplexity: 17.86\n",
            "Epoch [5/5], Step[800/1549], Loss: 2.9748, Perplexity: 19.59\n",
            "Epoch [5/5], Step[900/1549], Loss: 2.7764, Perplexity: 16.06\n",
            "Epoch [5/5], Step[1000/1549], Loss: 2.8969, Perplexity: 18.12\n",
            "Epoch [5/5], Step[1100/1549], Loss: 2.9826, Perplexity: 19.74\n",
            "Epoch [5/5], Step[1200/1549], Loss: 2.8895, Perplexity: 17.98\n",
            "Epoch [5/5], Step[1300/1549], Loss: 2.6705, Perplexity: 14.45\n",
            "Epoch [5/5], Step[1400/1549], Loss: 2.3726, Perplexity: 10.73\n",
            "Epoch [5/5], Step[1500/1549], Loss: 2.9207, Perplexity: 18.55\n"
          ]
        }
      ],
      "source": [
        "model = RNNLM(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Truncated backpropagation\n",
        "def detach(states):\n",
        "    return [state.detach() for state in states] \n",
        "\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    # Set initial hidden and cell states\n",
        "    states = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n",
        "              torch.zeros(num_layers, batch_size, hidden_size).to(device))\n",
        "    \n",
        "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
        "        # Get mini-batch inputs and targets\n",
        "        inputs = ids[:, i:i+seq_length].to(device)\n",
        "        targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        states = detach(states)\n",
        "        outputs, states = model(inputs, states)\n",
        "        loss = criterion(outputs, targets.reshape(-1))\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        step = (i+1) // seq_length\n",
        "        if step % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
        "                   .format(epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Vy9OJMEXRJs"
      },
      "source": [
        "# ü§î Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhis12qSX-ce"
      },
      "source": [
        "## 1Ô∏è‚É£ Q2.1 Detaching or not? (10 points)\n",
        "The above code implements a version of truncated backpropagation through time. The implementation only requires the `detach()` function (lines 7-9 of the cell) defined above the loop and used once inside the training loop.\n",
        "* Explain the implementation (compared to not using truncated backprop through time).\n",
        "* What does the `detach()` call here achieve? Draw a computational graph. You may choose to answer this question outside the notebook.\n",
        "* When using using line 7-9 we will typically observe less GPU memory being used during training, explain why in your answer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answered in the above PDF. Outside of the notebook.\n"
          ]
        }
      ],
      "source": [
        "print (\"Answered in the above PDF. Outside of the notebook.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbyKZiTgahSv"
      },
      "source": [
        "## üîÆ Model Prediction\n",
        "Below we will use our model to generate text sequence!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DxQ13QcIjPE9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "had little more than N years \n",
            "but the report 's current account seems to be this is likely to <unk> because it tends to enter out office of the cars from the u.s. \n",
            "cancer patients clearly shortages giving the studio mr. reitman 's \n",
            "it is n't starting \n"
          ]
        }
      ],
      "source": [
        "# Sample from the model\n",
        "with torch.no_grad():\n",
        "    with open('sample.txt', 'w') as f:\n",
        "        # Set intial hidden ane cell states\n",
        "        state = (torch.zeros(num_layers, 1, hidden_size).to(device),\n",
        "                 torch.zeros(num_layers, 1, hidden_size).to(device))\n",
        "\n",
        "        # Select one word id randomly\n",
        "        prob = torch.ones(vocab_size)\n",
        "        input = torch.multinomial(prob, num_samples=1).unsqueeze(1).to(device)\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            # Forward propagate RNN \n",
        "            output, state = model(input, state)\n",
        "\n",
        "            # Sample a word id\n",
        "            prob = output.exp()\n",
        "            word_id = torch.multinomial(prob, num_samples=1).item()\n",
        "\n",
        "            # Fill input with sampled word id for the next time step\n",
        "            input.fill_(word_id)\n",
        "\n",
        "            # File write\n",
        "            word = corpus.dictionary.idx2word[word_id]\n",
        "            word = '\\n' if word == '<eos>' else word + ' '\n",
        "            f.write(word)\n",
        "\n",
        "            if (i+1) % 100 == 0:\n",
        "                print('Sampled [{}/{}] words and save to {}'.format(i+1, num_samples, 'sample.txt'))\n",
        "! type sample.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXsUDt0tbAHM"
      },
      "source": [
        "## 2Ô∏è‚É£ Q2.2 Sampling strategy (7 points)\n",
        "Consider the sampling procedure above. The current code samples a word:\n",
        "```python\n",
        "word_id = torch.multinomial(prob, num_samples=1).item()\n",
        "```\n",
        "in order to feed the model at each output step and feeding those to the next timestep. Copy below the above cell and modify this sampling startegy to use a greedy sampling which selects the highest probability word at each time step to feed as the next input."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "2BeO7LSWiyIZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'s unexpected strengthening on the economy and the economy \n",
            "the index which uses the dollar began at N down N \n",
            "the index was N N \n",
            "the index registered N in august compared with N in july and N \n",
            "the index registered N in august with the \n"
          ]
        }
      ],
      "source": [
        "# Sample greedily from the model\n",
        "# Sample from the model using greedy sampling\n",
        "with torch.no_grad():\n",
        "    with open('sample_greedy.txt', 'w') as f:\n",
        "        # Set initial hidden and cell states\n",
        "        state = (torch.zeros(num_layers, 1, hidden_size).to(device),\n",
        "                 torch.zeros(num_layers, 1, hidden_size).to(device))\n",
        "\n",
        "        # Select one word id randomly\n",
        "        prob = torch.ones(vocab_size)\n",
        "        input = torch.multinomial(prob, num_samples=1).unsqueeze(1).to(device)\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            # Forward propagate RNN \n",
        "            output, state = model(input, state)\n",
        "\n",
        "            # Sample a word id using greedy approach\n",
        "            prob = output.exp()\n",
        "            word_id = torch.argmax(prob, dim=-1).item()\n",
        "\n",
        "            # Fill input with sampled word id for the next time step\n",
        "            input.fill_(word_id)\n",
        "\n",
        "            # File write\n",
        "            word = corpus.dictionary.idx2word[word_id]\n",
        "            word = '\\n' if word == '<eos>' else word + ' '\n",
        "            f.write(word)\n",
        "\n",
        "            if (i+1) % 100 == 0:\n",
        "                print('Sampled [{}/{}] words and save to {}'.format(i+1, num_samples, 'sample_greedy.txt'))\n",
        "! type sample_greedy.txt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8YV7laBe9er"
      },
      "source": [
        "## 3Ô∏è‚É£ Q2.3 Embedding Distance (8 points)\n",
        "Our model has learned a specific set of word embeddings.\n",
        "* Write a function that takes in 2 words and prints the cosine distance between their embeddings using the word embeddings from the above models.\n",
        "* Use it to print the cosine distance of the word \"army\" and the word \"taxpayer\".\n",
        "\n",
        "*Refer to the sampling code for how to output the words corresponding to each index. To get the index you can use the function `corpus.dictionary.word2idx.`*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "e6w3JSY3d_6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The cosine distance between 'army' and 'taxpayer' is 1.1022\n"
          ]
        }
      ],
      "source": [
        "# Embedding distance\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def get_cosine_distance(a, b,model):\n",
        "    idx1 = corpus.dictionary.word2idx[a]\n",
        "    idx2 = corpus.dictionary.word2idx[b]\n",
        "\n",
        "    embed1 = model.embed.weight[idx1]\n",
        "    embed2 = model.embed.weight[idx2]\n",
        "\n",
        "    similarity = F.cosine_similarity(embed1.unsqueeze(0), embed2.unsqueeze(0))\n",
        "\n",
        "    distance = 1 - similarity.item()\n",
        "\n",
        "    return distance\n",
        "\n",
        "word1 = 'army'\n",
        "word2 = 'taxpayer'\n",
        "distance = get_cosine_distance(word1, word2, model)\n",
        "print(f\"The cosine distance between '{word1}' and '{word2}' is {distance:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O44EBrsQdA4n"
      },
      "source": [
        "## 4Ô∏è‚É£ Q2.4 Teacher Forcing (Extra Credit 2 points)\n",
        "What is teacher forcing?\n",
        "> Teacher forcing works by using the actual or expected output from the training dataset at the current time step $y(t)$ as input in the next time step $X(t+1)$, rather than the output generated by the network.\n",
        "\n",
        "In the `üèì Training` code this is achieved, implicitly, when we pass the entire input sequence (`inputs = ids[:, i:i+seq_length].to(device)`) to the model at once.\n",
        "\n",
        "Copy below the `üèì Training` code and modify it to disable teacher forcing training. Compare the performance of this model, to original model, what can you conclude? (compare perplexity and convergence rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "qfgf5pJGfL-D"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Step[0/1549], Loss: 9.2097, Perplexity: 9993.40\n",
            "Epoch [1/5], Step[100/1549], Loss: 6.5312, Perplexity: 686.22\n",
            "Epoch [1/5], Step[200/1549], Loss: 6.6389, Perplexity: 764.24\n",
            "Epoch [1/5], Step[300/1549], Loss: 6.6714, Perplexity: 789.52\n",
            "Epoch [1/5], Step[400/1549], Loss: 6.5736, Perplexity: 715.95\n",
            "Epoch [1/5], Step[500/1549], Loss: 6.5178, Perplexity: 677.10\n",
            "Epoch [1/5], Step[600/1549], Loss: 6.4526, Perplexity: 634.37\n",
            "Epoch [1/5], Step[700/1549], Loss: 6.6863, Perplexity: 801.36\n",
            "Epoch [1/5], Step[800/1549], Loss: 6.4238, Perplexity: 616.33\n",
            "Epoch [1/5], Step[900/1549], Loss: 6.5778, Perplexity: 718.94\n",
            "Epoch [1/5], Step[1000/1549], Loss: 6.5673, Perplexity: 711.42\n",
            "Epoch [1/5], Step[1100/1549], Loss: 6.7199, Perplexity: 828.71\n",
            "Epoch [1/5], Step[1200/1549], Loss: 6.5152, Perplexity: 675.32\n",
            "Epoch [1/5], Step[1300/1549], Loss: 6.7478, Perplexity: 852.22\n",
            "Epoch [1/5], Step[1400/1549], Loss: 6.5587, Perplexity: 705.35\n",
            "Epoch [1/5], Step[1500/1549], Loss: 6.6037, Perplexity: 737.79\n",
            "Epoch [2/5], Step[0/1549], Loss: 8.1542, Perplexity: 3478.01\n",
            "Epoch [2/5], Step[100/1549], Loss: 6.3141, Perplexity: 552.29\n",
            "Epoch [2/5], Step[200/1549], Loss: 6.3861, Perplexity: 593.51\n",
            "Epoch [2/5], Step[300/1549], Loss: 6.5283, Perplexity: 684.22\n",
            "Epoch [2/5], Step[400/1549], Loss: 6.4733, Perplexity: 647.60\n",
            "Epoch [2/5], Step[500/1549], Loss: 6.2380, Perplexity: 511.85\n",
            "Epoch [2/5], Step[600/1549], Loss: 6.2412, Perplexity: 513.46\n",
            "Epoch [2/5], Step[700/1549], Loss: 6.4999, Perplexity: 665.05\n",
            "Epoch [2/5], Step[800/1549], Loss: 6.3046, Perplexity: 547.09\n",
            "Epoch [2/5], Step[900/1549], Loss: 6.4421, Perplexity: 627.74\n",
            "Epoch [2/5], Step[1000/1549], Loss: 6.4385, Perplexity: 625.46\n",
            "Epoch [2/5], Step[1100/1549], Loss: 6.5548, Perplexity: 702.58\n",
            "Epoch [2/5], Step[1200/1549], Loss: 6.4065, Perplexity: 605.79\n",
            "Epoch [2/5], Step[1300/1549], Loss: 6.5075, Perplexity: 670.12\n",
            "Epoch [2/5], Step[1400/1549], Loss: 6.4204, Perplexity: 614.22\n",
            "Epoch [2/5], Step[1500/1549], Loss: 6.4570, Perplexity: 637.18\n",
            "Epoch [3/5], Step[0/1549], Loss: 6.7246, Perplexity: 832.63\n",
            "Epoch [3/5], Step[100/1549], Loss: 6.2055, Perplexity: 495.49\n",
            "Epoch [3/5], Step[200/1549], Loss: 6.2376, Perplexity: 511.61\n",
            "Epoch [3/5], Step[300/1549], Loss: 6.4465, Perplexity: 630.50\n",
            "Epoch [3/5], Step[400/1549], Loss: 6.4238, Perplexity: 616.37\n",
            "Epoch [3/5], Step[500/1549], Loss: 6.1189, Perplexity: 454.37\n",
            "Epoch [3/5], Step[600/1549], Loss: 6.2208, Perplexity: 503.12\n",
            "Epoch [3/5], Step[700/1549], Loss: 6.4263, Perplexity: 617.90\n",
            "Epoch [3/5], Step[800/1549], Loss: 6.2033, Perplexity: 494.39\n",
            "Epoch [3/5], Step[900/1549], Loss: 6.3113, Perplexity: 550.74\n",
            "Epoch [3/5], Step[1000/1549], Loss: 6.3096, Perplexity: 549.85\n",
            "Epoch [3/5], Step[1100/1549], Loss: 6.3997, Perplexity: 601.69\n",
            "Epoch [3/5], Step[1200/1549], Loss: 6.2745, Perplexity: 530.87\n",
            "Epoch [3/5], Step[1300/1549], Loss: 6.3819, Perplexity: 591.05\n",
            "Epoch [3/5], Step[1400/1549], Loss: 6.3023, Perplexity: 545.81\n",
            "Epoch [3/5], Step[1500/1549], Loss: 6.2675, Perplexity: 527.17\n",
            "Epoch [4/5], Step[0/1549], Loss: 6.5475, Perplexity: 697.48\n",
            "Epoch [4/5], Step[100/1549], Loss: 6.1811, Perplexity: 483.54\n",
            "Epoch [4/5], Step[200/1549], Loss: 6.2515, Perplexity: 518.80\n",
            "Epoch [4/5], Step[300/1549], Loss: 6.3563, Perplexity: 576.14\n",
            "Epoch [4/5], Step[400/1549], Loss: 6.2971, Perplexity: 543.01\n",
            "Epoch [4/5], Step[500/1549], Loss: 6.0442, Perplexity: 421.66\n",
            "Epoch [4/5], Step[600/1549], Loss: 6.1266, Perplexity: 457.86\n",
            "Epoch [4/5], Step[700/1549], Loss: 6.3525, Perplexity: 573.91\n",
            "Epoch [4/5], Step[800/1549], Loss: 6.1367, Perplexity: 462.51\n",
            "Epoch [4/5], Step[900/1549], Loss: 6.2070, Perplexity: 496.20\n",
            "Epoch [4/5], Step[1000/1549], Loss: 6.2381, Perplexity: 511.89\n",
            "Epoch [4/5], Step[1100/1549], Loss: 6.3913, Perplexity: 596.66\n",
            "Epoch [4/5], Step[1200/1549], Loss: 6.2039, Perplexity: 494.66\n",
            "Epoch [4/5], Step[1300/1549], Loss: 6.3092, Perplexity: 549.63\n",
            "Epoch [4/5], Step[1400/1549], Loss: 6.1635, Perplexity: 475.06\n",
            "Epoch [4/5], Step[1500/1549], Loss: 6.1648, Perplexity: 475.70\n",
            "Epoch [5/5], Step[0/1549], Loss: 6.3928, Perplexity: 597.53\n",
            "Epoch [5/5], Step[100/1549], Loss: 6.1331, Perplexity: 460.84\n",
            "Epoch [5/5], Step[200/1549], Loss: 6.1405, Perplexity: 464.29\n",
            "Epoch [5/5], Step[300/1549], Loss: 6.3862, Perplexity: 593.62\n",
            "Epoch [5/5], Step[400/1549], Loss: 6.2252, Perplexity: 505.33\n",
            "Epoch [5/5], Step[500/1549], Loss: 5.8803, Perplexity: 357.91\n",
            "Epoch [5/5], Step[600/1549], Loss: 6.0875, Perplexity: 440.34\n",
            "Epoch [5/5], Step[700/1549], Loss: 6.2520, Perplexity: 519.07\n",
            "Epoch [5/5], Step[800/1549], Loss: 6.0414, Perplexity: 420.50\n",
            "Epoch [5/5], Step[900/1549], Loss: 6.3427, Perplexity: 568.33\n",
            "Epoch [5/5], Step[1000/1549], Loss: 6.1992, Perplexity: 492.36\n",
            "Epoch [5/5], Step[1100/1549], Loss: 6.3112, Perplexity: 550.68\n",
            "Epoch [5/5], Step[1200/1549], Loss: 6.1178, Perplexity: 453.89\n",
            "Epoch [5/5], Step[1300/1549], Loss: 6.2769, Perplexity: 532.12\n",
            "Epoch [5/5], Step[1400/1549], Loss: 6.1248, Perplexity: 457.07\n",
            "Epoch [5/5], Step[1500/1549], Loss: 6.0773, Perplexity: 435.85\n"
          ]
        }
      ],
      "source": [
        "# Training code without Teacher Forcing\n",
        "\n",
        "model = RNNLM(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Set initial hidden and cell states\n",
        "    states = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n",
        "              torch.zeros(num_layers, batch_size, hidden_size).to(device))\n",
        "    \n",
        "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
        "        # Get mini-batch inputs and targets\n",
        "        inputs = ids[:, i:i+seq_length].to(device)\n",
        "        targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
        "\n",
        "        loss = 0\n",
        "        \n",
        "        # Forward pass\n",
        "        for time_step in range(seq_length):\n",
        "            states = detach(states)\n",
        "            outputs, states = model(inputs, states)\n",
        "            reshaped_outputs = outputs.view(batch_size, -1, vocab_size)[:, -1, :]\n",
        "            current_loss = criterion(reshaped_outputs, targets[:, time_step].view(-1))\n",
        "            loss += current_loss\n",
        "\n",
        "            inputs = torch.argmax(reshaped_outputs, dim=-1).detach().unsqueeze(1)\n",
        "        \n",
        "        loss /= seq_length\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        step = (i+1) // seq_length\n",
        "        if step % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
        "                   .format(epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xH2TG4v3PBOu"
      },
      "source": [
        "## 5Ô∏è‚É£ Q2.5 Distance Comparison (+1 point)\n",
        "Repeat the work you did for `3Ô∏è‚É£ Q2.3 Embedding Distance` for the model in `4Ô∏è‚É£ Q2.4 Teacher Forcing` and compare the distances produced by these two models (i.e. with and without the teacher forcing), what can you conclude?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EABSoOAGPAaS"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The cosine distance between 'army' and 'taxpayer' is 0.9217\n"
          ]
        }
      ],
      "source": [
        "distance_without_teacher_forcing = get_cosine_distance(word1, word2, model)\n",
        "print(f\"The cosine distance between '{word1}' and '{word2}' is {distance_without_teacher_forcing:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
