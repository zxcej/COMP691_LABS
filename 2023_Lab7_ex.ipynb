{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zxcej/COMP691_LABS/blob/main/2023_Lab7_ex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMvnHJy2VAh7"
      },
      "source": [
        "# Lab 7: Self-Attention\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnc8ygodCf88"
      },
      "source": [
        "This lab covers the following topics:\n",
        "\n",
        "- Gain insight into the self-attention operation using the sequential MNIST example from before.\n",
        "- Gain insight into positional encodings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcGa4Zk7CSO6"
      },
      "source": [
        "## 0 Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nafW18o5aPa9"
      },
      "source": [
        "Run the code cell below to download the MNIST digits dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hov-36duZyzP"
      },
      "source": [
        "!wget -O MNIST.tar.gz https://activeeon-public.s3.eu-west-2.amazonaws.com/datasets/MNIST.new.tar.gz\n",
        "!tar -zxvf MNIST.tar.gz\n",
        "\n",
        "import torchvision\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "dataset = torchvision.datasets.MNIST('./', download=False, transform=transforms.Compose([transforms.ToTensor()]), train=True)\n",
        "train_indices = torch.arange(0, 10000)\n",
        "train_dataset = Subset(dataset, train_indices)\n",
        "\n",
        "dataset=torchvision.datasets.MNIST('./', download=False, transform=transforms.Compose([transforms.ToTensor()]), train=False)\n",
        "test_indices = torch.arange(0, 10000)\n",
        "test_dataset = Subset(dataset, test_indices)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16,\n",
        "                                          shuffle=False, num_workers=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJ-OsDod2fvG"
      },
      "source": [
        "## Exercise 1: Self-Attention without Positional Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5w7IP5RKSqOz"
      },
      "source": [
        "In this section, will implement a very simple model based on self-attention without positional encoding. The model you will implement will consider the input image as a sequence of 28 rows. You may use PyTorch's [`nn.MultiheadAttention`](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html) for this part. Implement a model with the following architecture:\n",
        "\n",
        "* **Input**: Input image of shape `(batch_size, sequence_length, input_size)`, where $\\text{sequence_length} = \\text{image_height}$ and $\\text{input_size} = \\text{image_width}$.\n",
        "\n",
        "* **Linear 1**: Linear layer which converts input of shape `(sequence_length*batch_size, input_size)` to input of shape `(sequence_length*batch_size, embed_dim)`, where `embed_dim` is the embedding dimension.\n",
        "\n",
        "* **Attention 1**: `nn.MultiheadAttention` layer with 8 heads which takes an input of shape `(sequence_length, batch_size, embed_dim)` and outputs a tensor of shape `(sequence_length, batch_size, embed_dim)`. \n",
        "\n",
        "* **ReLU**: ReLU activation layer.\n",
        "\n",
        "* **Linear 2**: Linear layer which converts input of shape `(sequence_length*batch_size, embed_dim)` to input of shape `(sequence_length*batch_size, embed_dim)`.\n",
        "\n",
        "* **ReLU**: ReLU activation layer.\n",
        "\n",
        "* **Attention 2**: `nn.MultiheadAttention` layer with 8 heads which takes an input of shape `(sequence_length, batch_size, embed_dim)` and outputs a tensor of shape `(sequence_length, batch_size, embed_dim)`.\n",
        "\n",
        "* **ReLU**: ReLU activation layer.\n",
        "\n",
        "* **AvgPool**: Average along the sequence dimension from `(batch_size, sequence_length, embed_dim)` to `(batch_size, embed_dim)`\n",
        "\n",
        "* **Linear 3**: Linear layer which takes an input of shape `(batch_size, embed_dim)` and outputs the class logits of shape `(batch_size, 10)`.\n",
        "\n",
        "\n",
        "**NOTE**: Be cautious of correctly permuting and reshaping the input between layers. E.g. if `x` is of shape `(batch_size, sequence_length, input_size)`, note that `x.reshape(batch_size*sequence_length, -1) != x.permute(1,0,2).reshape(batch_size*sequence_length, -1)`. In this example, `x.reshape(batch_size*sequence_length, -1)` has `[batch0_seq0, batch0_seq1, ..., batch1_seq0, batch1_seq1, ...]` format, while `x.permute(1,0,2).reshape(batch_size*sequence_length, -1)` has `[batch0_seq0, batch1_seq0, ..., batch0_seq1, batch1_seq1, ...]` format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppwhZ19Ff9FC"
      },
      "source": [
        "# Self-attention without positional encoding\n",
        "torch.manual_seed(691)\n",
        "\n",
        "# Define your model here\n",
        "class myModel(nn.Module):\n",
        "    def __init__(self, input_size, embed_dim, seq_length,\n",
        "                 num_classes=10, num_heads=8):\n",
        "        super(myModel, self).__init__()\n",
        "        # TODO: Initialize myModel\n",
        "        \n",
        "\n",
        "    def forward(self,x):\n",
        "        # TODO: Implement myModel forward pass\n",
        "        batch_size, sequence_length, input_size = x.shape\n",
        "        \n",
        "        pass\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Zu1c88kgKDZ"
      },
      "source": [
        "Train and evaluate your model by running the cell below. Expect to see  `60-80%` test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3FlSD16S8Nh"
      },
      "source": [
        "# Same training code \n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "sequence_length = 28\n",
        "input_size = 28\n",
        "hidden_size = 64\n",
        "num_layers = 2\n",
        "num_classes = 10\n",
        "num_epochs = 8\n",
        "learning_rate = 0.005\n",
        "\n",
        "# Initialize model\n",
        "model = myModel(input_size=input_size, embed_dim=hidden_size, seq_length=sequence_length)\n",
        "model = model.to(device)\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 10 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "\n",
        "# Test the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYeCqWt_BBRz"
      },
      "source": [
        "## Exercise 2: Self-Attention with Positional Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l71kP10-45YF"
      },
      "source": [
        "Implement a similar model to exercise 1, except this time your embedded input should be added with the positional encoding. For the purpose of this lab, we will use a learned positional encoding, which will be a trainable embedding. Your positional encodings will be added to the initial transformation of the input.\n",
        "\n",
        "* **Input**: Input image of shape `(batch_size, sequence_length, input_size)`, where $\\text{sequence_length} = \\text{image_height}$ and $\\text{input_size} = \\text{image_width}$.\n",
        "\n",
        "* **Linear 1**: Linear layer which converts input of shape `(batch_size*sequence_length, input_size)` to input of shape `(batch_size*sequence_length, embed_dim)`, where `embed_dim` is the embedding dimension.\n",
        "\n",
        "* **Add Positional Encoding**: Add a learnable positional encoding of shape `(sequence_length, batch_size, embed_dim)` to input of shape `(sequence_length, batch_size, embed_dim)`, where `pos_embed` is the positional embedding size. The output will be of shape `(sequence_length, batch_size, embed_dim)`.\n",
        "\n",
        "* **Attention 1**: `nn.MultiheadAttention` layer with 8 heads which takes an input of shape `(sequence_length, batch_size, embed_dim)` and outputs a tensor of shape `(sequence_length, batch_size, embed_dim)`.\n",
        "\n",
        "* **ReLU**: ReLU activation layer.\n",
        "\n",
        "* **Linear 2**: Linear layer which converts input of shape `(sequence_length*batch_size, features_dim)` to input of shape `(sequence_length*batch_size, features_dim)`.\n",
        "\n",
        "* **ReLU**: ReLU activation layer.\n",
        "\n",
        "* **Attention 2**: `nn.MultiheadAttention` layer with 8 heads which takes an input of shape `(sequence_length, batch_size, features_dim)` and outputs a tensor of shape `(sequence_length, batch_size, features_dim)`.\n",
        "\n",
        "* **ReLU**: ReLU activation layer.\n",
        "\n",
        "* **AvgPool**: Average along the sequence dimension from `(batch_size, sequence_length, features_dim)` to `(batch_size, features_dim)`\n",
        "\n",
        "* **Linear 3**: Linear layer which takes an input of shape `(batch_size, sequence_length*features_dim)` and outputs the class logits of shape `(batch_size, 10)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xAfP5H2_p6o"
      },
      "source": [
        "# Self-attention with positional encoding\n",
        "torch.manual_seed(691)\n",
        "\n",
        "# Define your model here\n",
        "class myModel(nn.Module):\n",
        "    def __init__(self, input_size, embed_dim, seq_length,\n",
        "                 num_classes=10, num_heads=8):\n",
        "        super(myModel, self).__init__()\n",
        "        # TODO: Initialize myModel\n",
        "        self.positional_encoding = nn.Parameter(torch.rand(self.seq_length, self.embed_dim))\n",
        "\n",
        "    def forward(self,x):\n",
        "        # TODO: Implement myModel forward pass\n",
        "        batch_size, sequence_length, input_size = x.shape\n",
        "        \n",
        "        pass\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3IKf9WDBS4Y"
      },
      "source": [
        "Use the same training code as the one from part 1 to train your model. You may copy the training loop here. Expect to see close to `~90+%` test accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxsHnOzXBk95"
      },
      "source": [
        "# Same training code \n",
        "\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "sequence_length = 28\n",
        "input_size = 28\n",
        "hidden_size = 64\n",
        "num_layers = 2\n",
        "num_classes = 10\n",
        "num_epochs = 8\n",
        "learning_rate = 0.005\n",
        "\n",
        "# Initialize model\n",
        "model = myModel(input_size=input_size, embed_dim=hidden_size, seq_length=sequence_length)\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 10 == 0:\n",
        "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "\n",
        "# Test the model\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}