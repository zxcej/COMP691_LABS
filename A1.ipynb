{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "\n",
    "### Exercise 1.(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Floating point error tolerance is:  1e-06\n",
      "Key is:  W1\n",
      "Difference is:  tensor(1.1212e-07, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Success in W1\n",
      "Key is:  b1\n",
      "Difference is:  tensor(5.2760e-08, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Success in b1\n",
      "Key is:  W2\n",
      "Difference is:  tensor(9.1290e-08, device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Success in W2\n",
      "Key is:  b2\n",
      "Difference is:  tensor(0., device='cuda:0', grad_fn=<LinalgVectorNormBackward0>)\n",
      "Success in b2\n"
     ]
    }
   ],
   "source": [
    "#1-hidden layer neural network y=w2^T*tanh(w1*x+b1)+b2\n",
    "#W1: 20 *10\n",
    "#W2: 1 * 20\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "\n",
    "#generate data\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "h_1 = 20\n",
    "h_2 = 1\n",
    "param_dict = {\"W1\": torch.randn(h_1,10,device=device,requires_grad=True),\n",
    "              \"b1\":torch.randn(h_1,1,device=device,requires_grad=True),\n",
    "              \"W2\":torch.randn(h_1,h_2,device=device,requires_grad=True),\n",
    "              \"b2\":torch.randn(1, device=device,requires_grad=True)}\n",
    "\n",
    "#generate data\n",
    "x = torch.randn(100,10,1,device=device)\n",
    "y = torch.randn(100,1,1,device=device)\n",
    "\n",
    "\n",
    "def my_nn(x,param_dict):\n",
    "    \n",
    "    x = x.clone().detach().requires_grad_(True)\n",
    "    x = torch.tanh(param_dict[\"W1\"]@x+param_dict[\"b1\"])\n",
    "    x = param_dict[\"W2\"].T@x+param_dict[\"b2\"]\n",
    "    return x\n",
    "\n",
    "def my_MAE(y_hat,y):\n",
    "    # abs value\n",
    "    return torch.mean(torch.abs(y_hat-y))\n",
    "\n",
    "def my_loss_grad(y_hat,y):\n",
    "    return torch.sign(y_hat-y)\n",
    "\n",
    "def my_nn_grad(x,y_hat,y,param_dict):\n",
    "\n",
    "    x = x.clone().detach().requires_grad_(True)\n",
    "    y_hat = y_hat.clone().detach().requires_grad_(True)\n",
    "    y = y.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    grad_dict = {}\n",
    "    mlg = my_loss_grad(y_hat,y)\n",
    "    grad_dict[\"b2\"] = torch.mean(mlg)\n",
    "    # my_loss_grad(y_hat,y) is 100*1*1\n",
    "    # torch.tanh(param_dict[\"W1\"]@x+param_dict[\"b1\"]) is 100*20*1\n",
    "    pre_w2 = mlg * torch.tanh(param_dict[\"W1\"]@x+param_dict[\"b1\"])\n",
    "    grad_dict[\"W2\"] = torch.mean(pre_w2,dim=0)\n",
    "    pre_b1 = mlg * param_dict[\"W2\"] * (1-torch.tanh(param_dict[\"W1\"]@x+param_dict[\"b1\"])**2)\n",
    "    grad_dict[\"b1\"] = torch.mean(pre_b1,dim=0)\n",
    "    pre_w1 = pre_b1 @ x.permute(0,2,1)\n",
    "    grad_dict[\"W1\"] = torch.mean(pre_w1,dim=0)\n",
    "\n",
    "    return grad_dict\n",
    "\n",
    "def torch_grad_check(x,y,param_dict,eps=1e-6):\n",
    "    # check my_nn_grad and torch.autograd.grad\n",
    "    print(\"Floating point error tolerance is: \", eps)\n",
    "    y_hat = my_nn(x,param_dict)\n",
    "    #backwards\n",
    "    loss = my_MAE(y_hat,y)\n",
    "    loss.backward()\n",
    "    grad_dict = my_nn_grad(x,y_hat,y,param_dict)\n",
    "    for key in param_dict.keys():\n",
    "        #use torch.linalg.norm to compare if smaller than eps\n",
    "        print (\"Key is: \", key)\n",
    "        #print (\"My grad is: \", grad_dict[key])\n",
    "        #print (\"Torch grad is: \", param_dict[key].grad)\n",
    "        diff = torch.linalg.norm(grad_dict[key]-param_dict[key].grad)\n",
    "        print (\"Difference is: \", diff)\n",
    "        #assert diff < eps, print error message if failed, print success message if passed\n",
    "        assert diff < eps, \"error in \"+key\n",
    "        print(\"Success in \"+key)\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "#check my_nn_grad and torch.autograd\n",
    "torch_grad_check(x,y,param_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 Train loss:  4.929087238547242 Val loss:  3.2611297332879268 Test loss:  3.2259187286282764\n",
      "Epoch:  2 Train loss:  2.0934204860233967 Val loss:  1.3236310229156956 Test loss:  1.2724719422834891\n",
      "Epoch:  3 Train loss:  0.9945494738625892 Val loss:  0.8580863909287886 Test loss:  0.8100081691403448\n",
      "Epoch:  4 Train loss:  0.7692363986998428 Val loss:  0.7691951457298163 Test loss:  0.7115339984496435\n",
      "Epoch:  5 Train loss:  0.7048417037283933 Val loss:  0.7310889318133845 Test loss:  0.6757433416298878\n",
      "Epoch:  6 Train loss:  0.6804037222891678 Val loss:  0.7329685986042023 Test loss:  0.6562707655959659\n",
      "Epoch:  7 Train loss:  0.6602157961439203 Val loss:  0.7022379772229628 Test loss:  0.6451992902122898\n",
      "Epoch:  8 Train loss:  0.6530472982076951 Val loss:  0.6851504544417063 Test loss:  0.6329366741357026\n",
      "Epoch:  9 Train loss:  0.6383828653229607 Val loss:  0.6795233605485974 Test loss:  0.6215177156307079\n",
      "Epoch:  10 Train loss:  0.6264210519967256 Val loss:  0.6677028925129862 Test loss:  0.6156922966977696\n",
      "Epoch:  11 Train loss:  0.615215688759898 Val loss:  0.6538444044012012 Test loss:  0.6058884496674125\n",
      "Epoch:  12 Train loss:  0.6071369070329784 Val loss:  0.6512647867202759 Test loss:  0.5932263173622849\n",
      "Epoch:  13 Train loss:  0.5983900308241079 Val loss:  0.6371464530626932 Test loss:  0.5864615732872928\n",
      "Epoch:  14 Train loss:  0.590783123800784 Val loss:  0.6269838376478716 Test loss:  0.5821182190636058\n",
      "Epoch:  15 Train loss:  0.5825630855045201 Val loss:  0.6206584392171918 Test loss:  0.5731463334810587\n",
      "Epoch:  16 Train loss:  0.5763818516775414 Val loss:  0.6163747012615204 Test loss:  0.5658307592442007\n",
      "Epoch:  17 Train loss:  0.5733596456271631 Val loss:  0.6078924663139113 Test loss:  0.5595158147223201\n",
      "Epoch:  18 Train loss:  0.5686885095488878 Val loss:  0.5975427121827097 Test loss:  0.5524194058444765\n",
      "Epoch:  19 Train loss:  0.5597884316503265 Val loss:  0.5977984757134409 Test loss:  0.5473446540626479\n",
      "Epoch:  20 Train loss:  0.5515694982475705 Val loss:  0.5832528864795511 Test loss:  0.5431677727603618\n",
      "Epoch:  21 Train loss:  0.5472808257665163 Val loss:  0.5798993995695403 Test loss:  0.5387648616308047\n",
      "Epoch:  22 Train loss:  0.5408907402076839 Val loss:  0.5872596619707166 Test loss:  0.5324086851359885\n",
      "Epoch:  23 Train loss:  0.5367279199906337 Val loss:  0.5758551223711534 Test loss:  0.5290814235254571\n",
      "Epoch:  24 Train loss:  0.5334790770286395 Val loss:  0.5616383227435026 Test loss:  0.5237736350592271\n",
      "Epoch:  25 Train loss:  0.5303993063208498 Val loss:  0.5686253679521156 Test loss:  0.5243203657460801\n",
      "Epoch:  26 Train loss:  0.523615093142898 Val loss:  0.5604926112926367 Test loss:  0.5160394772703265\n",
      "Epoch:  27 Train loss:  0.5206114790505834 Val loss:  0.5599998380198623 Test loss:  0.5182537903756271\n",
      "Epoch:  28 Train loss:  0.5162358615133498 Val loss:  0.5497956275939941 Test loss:  0.512487001993038\n",
      "Epoch:  29 Train loss:  0.5129614935061078 Val loss:  0.551030158996582 Test loss:  0.5074087005154586\n",
      "Epoch:  30 Train loss:  0.5100467760621765 Val loss:  0.539705136960203 Test loss:  0.5046376887662911\n",
      "Epoch:  31 Train loss:  0.5062326259083219 Val loss:  0.5417977128968094 Test loss:  0.5011875073850891\n",
      "Epoch:  32 Train loss:  0.5039009604557061 Val loss:  0.541254873528625 Test loss:  0.4990890948676769\n",
      "Epoch:  33 Train loss:  0.5011722431690605 Val loss:  0.5381176340760607 Test loss:  0.49831418969013075\n",
      "Epoch:  34 Train loss:  0.5027913451194763 Val loss:  0.5404144417155873 Test loss:  0.4918439807715239\n",
      "Epoch:  35 Train loss:  0.4982623079860652 Val loss:  0.5269193324175748 Test loss:  0.4919375114970737\n",
      "Epoch:  36 Train loss:  0.4944172148351316 Val loss:  0.5240285423668948 Test loss:  0.4897510990684415\n",
      "Epoch:  37 Train loss:  0.491114044814934 Val loss:  0.5262007993279081 Test loss:  0.48964925045952384\n",
      "Epoch:  38 Train loss:  0.4901001144338537 Val loss:  0.5224104930054058 Test loss:  0.48457498590887327\n",
      "Epoch:  39 Train loss:  0.4871628515330362 Val loss:  0.5241849729509065 Test loss:  0.485031364692582\n",
      "Epoch:  40 Train loss:  0.4854979220731759 Val loss:  0.5191661661321466 Test loss:  0.4808118720481425\n"
     ]
    }
   ],
   "source": [
    "# Train this model on the sklearn California Housing Prices datasets\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html#sklearn.datasets.fetch_california_housing\n",
    "\n",
    "import scipy\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from torch.utils import data\n",
    "\n",
    "\n",
    "# Generate a random California housing dataset\n",
    "# half of the data is used for training, the other half for testing\n",
    "# the data is normalized to have zero mean and unit variance\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "    scaler.fit_transform(sklearn.datasets.fetch_california_housing().data),\n",
    "    sklearn.datasets.fetch_california_housing().target,\n",
    "    test_size=0.5,\n",
    "    random_state=42)\n",
    "\n",
    "train_batch_size = 64\n",
    "test_batch_size = 64\n",
    "\n",
    "# dataloader to gpu\n",
    "train_dataset = data.TensorDataset(torch.tensor(X_train,device=device, dtype=torch.float32), torch.tensor(y_train,device=device, dtype=torch.float32))\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "test_dataset = data.TensorDataset(torch.tensor(X_test,device=device, dtype=torch.float32), torch.tensor(y_test,device=device, dtype=torch.float32))\n",
    "test_loader = data.DataLoader(test_dataset, batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "#validation set from training set\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [int(0.8*len(train_dataset)), int(0.2*len(train_dataset))])\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=test_batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "# convenient xavier initialization, taking an existing dictionary of parameters\n",
    "\n",
    "def xavier_init(param_dict):\n",
    "    for key in param_dict.keys():\n",
    "        if \"W\" in key:\n",
    "            torch.nn.init.xavier_uniform_(param_dict[key])\n",
    "        else:\n",
    "            torch.nn.init.zeros_(param_dict[key])\n",
    "        param_dict[key].requires_grad_(True)\n",
    "    return param_dict\n",
    "\n",
    "num_epochs = 40\n",
    "optimizer = torch.optim.Adam(param_dict.values(), lr=0.001)\n",
    "lr_scehudler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "#change w1 to 20*8 instead of 20*10\n",
    "param_dict[\"W1\"] = torch.randn(h_1,8,device=device,requires_grad=True)\n",
    "param_dict = xavier_init(param_dict)\n",
    "\n",
    "\n",
    "train_loss_per_epoch = []\n",
    "val_loss_per_epoch = []\n",
    "test_loss_per_epoch = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for batch_idx, (x, y) in enumerate(train_loader):\n",
    "        y_hat = my_nn(x.T,param_dict)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        loss = loss_fn(y_hat,y)\n",
    "        train_loss += loss.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    train_loss_per_epoch.append(train_loss/len(train_loader))\n",
    "\n",
    "    #validation\n",
    "    val_loss = 0\n",
    "    for batch_idx, (x, y) in enumerate(val_loader):\n",
    "        y_hat = my_nn(x.T,param_dict)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        loss = loss_fn(y_hat,y)\n",
    "        val_loss += loss.item()\n",
    "    val_loss_per_epoch.append(val_loss/len(val_loader))\n",
    "    \n",
    "    #test\n",
    "    test_loss = 0\n",
    "    for batch_idx, (x, y) in enumerate(test_loader):\n",
    "        y_hat = my_nn(x.T,param_dict)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        loss = loss_fn(y_hat,y)\n",
    "        test_loss += loss.item()\n",
    "    test_loss_per_epoch.append(test_loss/len(test_loader))\n",
    "\n",
    "    lr_scehudler.step(val_loss_per_epoch[-1])\n",
    "\n",
    "    print(\"Epoch: \", epoch+1, \"Train loss: \", train_loss_per_epoch[-1], \"Val loss: \", val_loss_per_epoch[-1], \"Test loss: \", test_loss_per_epoch[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1000 iterations:  9.357636213302612\n",
      "Jacobian is correct\n"
     ]
    }
   ],
   "source": [
    "def generate_dict(L,D,K,P):\n",
    "    param_dict = {}\n",
    "    param_dict[\"W1\"] = torch.randn(K,D,device=device)\n",
    "    for i in range(2,L+1):\n",
    "        param_dict[\"W\"+str(i)] = torch.randn(K,K,device=device)\n",
    "    param_dict[\"WF\"] = torch.randn(P,K,device=device)\n",
    "    return param_dict\n",
    "\n",
    "def my_nn_2a(x,L,param_dict):\n",
    "    x = param_dict[\"W1\"]@x\n",
    "    for i in range(2,L+1):\n",
    "        x = param_dict[\"W\"+str(i)]@torch.tanh(x)\n",
    "    x = param_dict[\"WF\"]@torch.tanh(x)\n",
    "    return x\n",
    "\n",
    "# backward automatic differentiation from scratch\n",
    "# using the chain rule\n",
    "\n",
    "def my_backward_2a(x,L,param_dict):\n",
    "    P = param_dict[\"WF\"].shape[0]\n",
    "    D = x.shape[0]\n",
    "    #forward pass\n",
    "    x = x.clone().detach().requires_grad_(True)\n",
    "    x = param_dict[\"W1\"]@x\n",
    "    tanh_outputs =[x]\n",
    "    for i in range(2,L+1):\n",
    "        x = param_dict[\"W\"+str(i)]@torch.tanh(x)\n",
    "        tanh_outputs.append(x)\n",
    "    x = param_dict[\"WF\"]@torch.tanh(x)\n",
    "    #backward pass for jacobian\n",
    "    df_dx = torch.zeros((P,D),device=device)\n",
    "    df_intermediate = torch.diag(1-torch.tanh(tanh_outputs[-1])**2)\n",
    "    df_intermediate = param_dict[\"WF\"]@df_intermediate\n",
    "    for i in range(L,1,-1):\n",
    "        df_intermediate = df_intermediate@ param_dict[\"W\"+str(i)]@torch.diag(1-torch.tanh(tanh_outputs[i-2])**2)\n",
    "    df_intermediate = df_intermediate@param_dict[\"W1\"]\n",
    "    #copy df_intermediate to df_dx\n",
    "    df_dx = df_intermediate.clone().detach()\n",
    "    return df_dx\n",
    "\n",
    "\n",
    " \n",
    "    \n",
    "\n",
    "epsilon = 1e-2\n",
    "    \n",
    "\n",
    "    \n",
    "D = 2\n",
    "K= 30\n",
    "P = 10\n",
    "L = 10\n",
    "param_dict = generate_dict(L,D,K,P)\n",
    "\n",
    "start = time.time()\n",
    "for i in range(1000):\n",
    "    test = torch.randn(D,device=device,requires_grad=True)\n",
    "    my_J = my_backward_2a(test,L,param_dict)\n",
    "    J_autograd = torch.autograd.functional.jacobian(lambda x: my_nn_2a(x,L,param_dict),test,create_graph=True,strategy=\"reverse-mode\")\n",
    "    #assert, print two jacobians if they are not equal\n",
    "    assert torch.allclose(my_J,J_autograd,atol=epsilon), print(my_J,J_autograd)\n",
    "    #Get time taken\n",
    "end = time.time()\n",
    "print(\"Time taken for 1000 iterations: \", end-start) \n",
    "print (\"Jacobian is correct\")\n",
    "    \n",
    "\n",
    "    \n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 1000 iterations:  4.432066440582275\n",
      "Jacobian is correct\n"
     ]
    }
   ],
   "source": [
    "def my_forward_2b(x,L,param_dict):\n",
    "    x = param_dict[\"W1\"]@x\n",
    "    df_dx = param_dict[\"W1\"]\n",
    "    for i in range(2,L+1):\n",
    "        x = torch.tanh(x)\n",
    "        #foward automatic differentiation\n",
    "        df_intermediate = torch.diag(1-x**2)\n",
    "        df_dx = param_dict[\"W\"+str(i)]@df_intermediate@df_dx\n",
    "        x = param_dict[\"W\"+str(i)]@x\n",
    "    x = torch.tanh(x)\n",
    "    df_intermediate = torch.diag(1-x**2)\n",
    "    df_dx = param_dict[\"WF\"]@df_intermediate@df_dx\n",
    "    x = param_dict[\"WF\"]@x\n",
    "    return df_dx\n",
    "\n",
    "param_dict = generate_dict(L,D,K,P)\n",
    "\n",
    "start = time.time()\n",
    "for i in range(1000):\n",
    "    test_2b = torch.randn(D,device=device,requires_grad=True)\n",
    "    my_J_2b = my_forward_2b(test_2b,L,param_dict)\n",
    "    J_autograd_2b = torch.autograd.functional.jacobian(lambda x: my_nn_2a(x,L,param_dict),test_2b, strategy=\"forward-mode\", vectorize=True)\n",
    "    #assert, print two jacobians if they are not equal\n",
    "    assert torch.allclose(my_J_2b,J_autograd_2b,atol=epsilon), print(my_J_2b,J_autograd_2b)\n",
    "end = time.time()\n",
    "print(\"Time taken for 1000 iterations: \", end-start)\n",
    "print (\"Jacobian is correct\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on GPU:  True\n",
      "L =  3\n",
      "Time taken for 10000 iterations of backward with L =  3  is  5.555159330368042\n",
      "Time taken for 10000 iterations of forward with L =  3  is  4.441255569458008\n",
      "L =  5\n",
      "Time taken for 10000 iterations of backward with L =  5  is  8.366724491119385\n",
      "Time taken for 10000 iterations of forward with L =  5  is  6.997880697250366\n",
      "L =  10\n",
      "Time taken for 10000 iterations of backward with L =  10  is  15.488043308258057\n",
      "Time taken for 10000 iterations of forward with L =  10  is  13.669673681259155\n"
     ]
    }
   ],
   "source": [
    "#Run on GPU\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Running on GPU: \", torch.cuda.is_available())\n",
    "\n",
    "test_l = [3,5,10]\n",
    "\n",
    "for L in test_l:\n",
    "    print(\"L = \", L)\n",
    "    param_dict = generate_dict(L,D,K,P)\n",
    "    start = time.time()\n",
    "    for i in range(10000):\n",
    "        test = torch.randn(D,device=device,requires_grad=True)\n",
    "        my_J = my_backward_2a(test,L,param_dict)\n",
    "    print (\"Time taken for 10000 iterations of backward with L = \", L, \" is \", time.time()-start)\n",
    "    start = time.time()\n",
    "    for i in range(10000):\n",
    "        test_2b = torch.randn(D,device=device,requires_grad=True)\n",
    "        my_J_2b = my_forward_2b(test_2b,L,param_dict)\n",
    "    print (\"Time taken for 10000 iterations of forward with L = \", L, \" is \", time.time()-start)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU\n",
      "L =  3\n",
      "Time taken for 10000 iterations of backward with L =  3  is  2.355909585952759\n",
      "Time taken for 10000 iterations of forward with L =  3  is  2.016078472137451\n",
      "L =  5\n",
      "Time taken for 10000 iterations of backward with L =  5  is  3.727365493774414\n",
      "Time taken for 10000 iterations of forward with L =  5  is  3.2781364917755127\n",
      "L =  10\n",
      "Time taken for 10000 iterations of backward with L =  10  is  6.791688680648804\n",
      "Time taken for 10000 iterations of forward with L =  10  is  6.134467840194702\n"
     ]
    }
   ],
   "source": [
    "#Run on GPU\n",
    "device = \"cpu\"\n",
    "print(\"Running on CPU\")\n",
    "\n",
    "test_l = [3,5,10]\n",
    "\n",
    "for L in test_l:\n",
    "    print(\"L = \", L)\n",
    "    param_dict = generate_dict(L,D,K,P)\n",
    "    start = time.time()\n",
    "    for i in range(10000):\n",
    "        test = torch.randn(D,device=device,requires_grad=True)\n",
    "        my_J = my_backward_2a(test,L,param_dict)\n",
    "    print (\"Time taken for 10000 iterations of backward with L = \", L, \" is \", time.time()-start)\n",
    "    start = time.time()\n",
    "    for i in range(10000):\n",
    "        test_2b = torch.randn(D,device=device,requires_grad=True)\n",
    "        my_J_2b = my_forward_2b(test_2b,L,param_dict)\n",
    "    print (\"Time taken for 10000 iterations of forward with L = \", L, \" is \", time.time()-start)\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63075362c8d96c0694eea802e32384973fa14ab497a9d2cc1491a70294264df6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
