{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "48d33845788e40b7a5185f18b80bf0ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bdd3f08a72f74a47a0a4fa326ba98bba",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3dae6a5a03a44607a18909460d85b4d6",
              "IPY_MODEL_7582bb5f909e45d78e248fc3c34a4535",
              "IPY_MODEL_056b155d7aac48c09d6d97ea1ea3c6f7"
            ]
          }
        },
        "bdd3f08a72f74a47a0a4fa326ba98bba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3dae6a5a03a44607a18909460d85b4d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_106716dec28142978e00f3f459a69214",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Epoch: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_284dc766d41945828354e782e515319c"
          }
        },
        "7582bb5f909e45d78e248fc3c34a4535": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b94e02d0a4d2493ba1206be86de1410f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 3,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 3,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a4d95243125d4d8fa9ccc6db7896a8aa"
          }
        },
        "056b155d7aac48c09d6d97ea1ea3c6f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8b998d1186954872b81d097fdddd4336",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 3/3 [00:14&lt;00:00,  4.89s/it, Loss/Train=0.395, Loss/Test=0.353, Acc/Test=84.5]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2be8a70acf324491aae37716eb6ccc2d"
          }
        },
        "106716dec28142978e00f3f459a69214": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "284dc766d41945828354e782e515319c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b94e02d0a4d2493ba1206be86de1410f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a4d95243125d4d8fa9ccc6db7896a8aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8b998d1186954872b81d097fdddd4336": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2be8a70acf324491aae37716eb6ccc2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zxcej/COMP691_LABS/blob/main/Lab8_HuggingFace_Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🤗 A Gentle Introduction to HuggingFace (HF)\n",
        "---\n",
        "HuggingFace provides you with a variety of pretrained models and\n",
        "functionalities to train/fine-tune these models and make inferences.\n",
        "\n",
        "Their [datasets](https://huggingface.co/docs/datasets/quickstart) library gives you access to many common NLP datasets. You can visualize these datasets on their [platform](https://huggingface.co/datasets) to get a sense of the data you would be working with."
      ],
      "metadata": {
        "id": "f2VA1IOI2z2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers"
      ],
      "metadata": {
        "id": "GQ3SRO_uAvYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🌠 Our Goal\n",
        "Our goal for this tutorial is to get familiar with the [transformers](https://huggingface.co/docs/transformers/index) library from HuggingFace and use a pretrained model to fine-tune it on a sequece classification task. More specifically we will fine-tune a [BERT](https://arxiv.org/pdf/1810.04805.pdf) model on the [Amazon Polarity](https://huggingface.co/datasets/amazon_polarity#data-instances) dataset.\n",
        "> The Amazon reviews dataset consists of reviews from amazon. The data span a period of 18 years, including ~35 million reviews up to March 2013. Reviews include product and user information, ratings, and a plaintext review.\n",
        "\n",
        "> The Amazon reviews polarity dataset is constructed by taking review score 1 and 2 as negative, and 4 and 5 as positive. Samples of score 3 is ignored. Each class has 1,800,000 training samples and 200,000 testing samples.\n",
        "\n",
        "Since the dataset is quite large, we will be working with only a subset of this dataset throughout this tutorial.\n"
      ],
      "metadata": {
        "id": "OV1La_Z59UiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🪜 Main Components\n",
        "The main components we would need to develop to realize our goal are:\n",
        "\n",
        "1. Load the data and make a dataset object for this task.\n",
        "2. Write a collate function/class to tokenize/transform/truncate batches of inputs.\n",
        "3. Make a custom model, which uses a pretrained model as its backbone and it is designed for our current task at hand.\n",
        "4. Write the training loop and train the model.\n",
        "\n",
        "> ⚠️ These steps constitues the basic building blocks to solve any other problem using HF."
      ],
      "metadata": {
        "id": "YyytVSoo9Waq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🛒 Loading data\n",
        "In this stage we will load the data from the `datasets` library. We will only load a small subset of the original dataset here in order to reduce the training time, but feel free to run this code on the full dataset on your own time and experiment with it.\n"
      ],
      "metadata": {
        "id": "hvJTeNSc9OR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_train = load_dataset(\"amazon_polarity\", split=\"train[:1000]\")\n",
        "dataset_test = load_dataset(\"amazon_polarity\", split=\"test[:200]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WI2ee9F3dft",
        "outputId": "b455dd4c-3690-442a-e111-ecad3e899c8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset amazon_polarity (/root/.cache/huggingface/datasets/amazon_polarity/amazon_polarity/3.0.0/56923eeb72030cb6c4ea30c8a4e1162c26b25973475ac1f44340f0ec0f2936f4)\n",
            "Reusing dataset amazon_polarity (/root/.cache/huggingface/datasets/amazon_polarity/amazon_polarity/3.0.0/56923eeb72030cb6c4ea30c8a4e1162c26b25973475ac1f44340f0ec0f2936f4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🔍 Quick look at the data { run: \"auto\" }\n",
        "#@markdown Lets have quick look at a few samples as well as the label distributions in our train and test set.\n",
        "n_samples_to_see = 3 #@param {type: \"integer\"}\n",
        "for i in range(n_samples_to_see):\n",
        "  print(\"-\"*30)\n",
        "  print(\"title:\", dataset_test[i][\"title\"])\n",
        "  print(\"content:\", dataset_test[i][\"content\"])\n",
        "  print(\"label:\", dataset_test[i][\"label\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o9vWB3lbI3p2",
        "outputId": "ce62853b-91c5-453b-e3e9-062d2313c211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------\n",
            "title: Great CD\n",
            "content: My lovely Pat has one of the GREAT voices of her generation. I have listened to this CD for YEARS and I still LOVE IT. When I'm in a good mood it makes me feel better. A bad mood just evaporates like sugar in the rain. This CD just oozes LIFE. Vocals are jusat STUUNNING and lyrics just kill. One of life's hidden gems. This is a desert isle CD in my book. Why she never made it big is just beyond me. Everytime I play this, no matter black, white, young, old, male, female EVERYBODY says one thing \"Who was that singing ?\"\n",
            "label: 1\n",
            "------------------------------\n",
            "title: One of the best game music soundtracks - for a game I didn't really play\n",
            "content: Despite the fact that I have only played a small portion of the game, the music I heard (plus the connection to Chrono Trigger which was great as well) led me to purchase the soundtrack, and it remains one of my favorite albums. There is an incredible mix of fun, epic, and emotional songs. Those sad and beautiful tracks I especially like, as there's not too many of those kinds of songs in my other video game soundtracks. I must admit that one of the songs (Life-A Distant Promise) has brought tears to my eyes on many occasions.My one complaint about this soundtrack is that they use guitar fretting effects in many of the songs, which I find distracting. But even if those weren't included I would still consider the collection worth it.\n",
            "label: 1\n",
            "------------------------------\n",
            "title: Batteries died within a year ...\n",
            "content: I bought this charger in Jul 2003 and it worked OK for a while. The design is nice and convenient. However, after about a year, the batteries would not hold a charge. Might as well just get alkaline disposables, or look elsewhere for a charger that comes with batteries that have better staying power.\n",
            "label: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def label_stats(ds):\n",
        "    negative = 0\n",
        "    positive = 0\n",
        "    for i in range(ds.num_rows):\n",
        "        if ds[i][\"label\"] == 1:\n",
        "            positive += 1\n",
        "        else:\n",
        "            negative += 1\n",
        "    return positive, negative"
      ],
      "metadata": {
        "id": "lgZxAErzE6PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, ds in enumerate([dataset_train, dataset_test]):\n",
        "    positive, negative = label_stats(ds)\n",
        "    if i == 0:\n",
        "        str_indicator = \"train\"\n",
        "    else:\n",
        "        str_indicator = \"test\"\n",
        "    print(\"+-\" * 15)\n",
        "    print(\"Set:\", str_indicator)\n",
        "    print(f\"Positive samples: {positive}\\nNegative samples: {negative}\")\n",
        "    print(f\"Percentage of overall positive samples: {(positive*100.0)/(positive+negative)}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIFY0vwJK5hx",
        "outputId": "8a573d2c-897f-4b4f-e8a1-aa9ee576e3aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
            "Set: train\n",
            "Positive samples: 462\n",
            "Negative samples: 538\n",
            "Percentage of overall positive samples: 46.2%\n",
            "+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-\n",
            "Set: test\n",
            "Positive samples: 109\n",
            "Negative samples: 91\n",
            "Percentage of overall positive samples: 54.5%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🧲 Collate\n",
        "Collate is a function that is called on every batch of data prepared by the [dataloader](https://https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). Once we pass our dataset (e.g. `train_set`) to our dataloader, each batch will be a `list` of `dict` items. Therefore, this cannot be directed to the model. We need to perform the followings at this stage:\n"
      ],
      "metadata": {
        "id": "80mbC9f8Q8au"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1️⃣ Tokenize the `text`\n",
        "Tokenize the `text`portion of each sample (i.e. parsing the text to smaller chuncks). Tokenization can happen in many ways, traditionally this was done based the white spaces. With transformer-based models tokenization is performed based on the frequency of occurance of \"chunk of text\". This frequence can be learnt in many different ways, however the most common one is the [**wordpiece**](https://arxiv.org/pdf/1609.08144v2.pdf) model. \n",
        "> The wordpiece model is generated using a data-driven approach to maximize the language-model likelihood\n",
        "of the training data, given an evolving word definition. Given a training corpus and a number of desired\n",
        "tokens $D$, the optimization problem is to select $D$ wordpieces such that the resulting corpus is minimal in the\n",
        "number of wordpieces when segmented according to the chosen wordpiece model.\n",
        "\n",
        "Under this model:\n",
        "1. Not all things can be converted to tokens depending on the model. For example, most models have been pretrained without any knowledge of emojis. So their token will be `[UNK]`, which stands for unknown.\n",
        "2. Some words will be mapped to multiple tokens!\n",
        "3. Depending on the kind of model, your tokens may or may not respect capitalization!"
      ],
      "metadata": {
        "id": "fzxwRDQFUtaG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "id": "2Go_MrtGVR8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🔍 Quick look at tokenization { run: \"auto\", vertical-output: true }\n",
        "input_sample = \"We are very jubilant to demonstrate to you the 🤗 Transformers library.\" #@param {type: \"string\"}\n",
        "tokenizer.tokenize(input_sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iu_rQoerVvsu",
        "outputId": "62731c30-338a-4bbf-b0ea-ce5aa6bf89a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['we',\n",
              " 'are',\n",
              " 'very',\n",
              " 'ju',\n",
              " '##bil',\n",
              " '##ant',\n",
              " 'to',\n",
              " 'demonstrate',\n",
              " 'to',\n",
              " 'you',\n",
              " 'the',\n",
              " 'transformers',\n",
              " 'library',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 236
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2️⃣ Encoding\n",
        "Once we have tokenized the text, we then need to convert these chuncks to numbers so we can feed them to our model. This conversion is basically a look-up in a dictionary **from `str` $\\to$ `int`**. The tokenizer object can also perform this work. While it does so it will also add the *special* tokens needed by the model to the encodings. "
      ],
      "metadata": {
        "id": "NEu6aqReXqp6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🔍 Quick look at token encoding { run: \"auto\"}\n",
        "input_sample = \"We are very jubilant to demonstrate to you the 🤗 Transformers library.\" #@param {type: \"string\"}\n",
        "print(\"--> Token Encodings:\\n\",tokenizer.encode(input_sample))\n",
        "print(\"-.\"*15)\n",
        "print(\"--> Token Encodings Decoded:\\n\",tokenizer.decode(tokenizer.encode(input_sample)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpDGccrvYKnT",
        "outputId": "79f6a6f3-93bb-4306-f5f2-2ef7a5030b97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Token Encodings:\n",
            " [101, 2057, 2024, 2200, 18414, 14454, 4630, 2000, 10580, 2000, 2017, 1996, 19081, 3075, 1012, 102]\n",
            "-.-.-.-.-.-.-.-.-.-.-.-.-.-.-.\n",
            "--> Token Encodings Decoded:\n",
            " [CLS] we are very jubilant to demonstrate to you the transformers library. [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3️⃣ Truncate/Pad samples\n",
        "Since all the sample in the batch will not have the same sequence length, we would need to truncate the longers (i.e. the ones that exeed a predefined maximum length) and pad the shorter ones so we that we can equal length for all the samples in the batch. Once this is achieved, we would need to convert the result to `torch.Tensor`s and return. These tensors will then be retrieved from the [dataloader](https://https//pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)."
      ],
      "metadata": {
        "id": "DI8lFKZSZ2ZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Union\n",
        "import torch\n",
        "\n",
        "\n",
        "class Collate:\n",
        "    def __init__(self, tokenizer: str, max_len: int) -> None:\n",
        "        self.tokenizer_name = tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __call__(self, batch: List[Dict[str, Union[str, int]]]) -> Dict[str, torch.Tensor]:\n",
        "        texts = list(map(lambda batch_instance: batch_instance[\"title\"], batch))\n",
        "        tokenized_inputs = self.tokenizer(\n",
        "            texts,\n",
        "            padding=\"longest\",\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_tensors=\"pt\",\n",
        "            return_token_type_ids=False,\n",
        "        )\n",
        "        labels = list(map(lambda batch_instance: int(batch_instance[\"label\"]), batch))\n",
        "        labels = torch.LongTensor(labels)\n",
        "        return dict(tokenized_inputs, **{\"labels\": labels})"
      ],
      "metadata": {
        "id": "SP31MsbHZxp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🧑‍🍳 Setting up the collate function { run: \"auto\" }\n",
        "tokenizer_name = \"distilbert-base-uncased\" #@param {type: \"string\"}\n",
        "sample_max_length = 64 #@param {type:\"slider\", min:32, max:512, step:1}\n",
        "collate = Collate(tokenizer=\"distilbert-base-uncased\", max_len=sample_max_length)"
      ],
      "metadata": {
        "id": "4VaSpuyIjNqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🤖 Model\n",
        "Our model needs to classify an entire sequence of text. Once we feed an input sequence of length $k$ to a language model, it will output $k$ vectors. Now the question is which of these vectors or combition of these vectors should we use to classify the sequence?\n",
        "We will use the first toke, special token `[cls]` for these purposes. *Refer to the [BERT paper](https://arxiv.org/abs/1810.04805) for more information.*\n",
        "\n",
        "Since we have 2 classes (positive, and negative), this means we would need to make a classifier on top of the vector representations of the `[cls]` token. Our custom model will then look like:"
      ],
      "metadata": {
        "id": "wG_15G_0dSyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModel\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "\n",
        "class ReviewClassifier(torch.nn.Module):\n",
        "    def __init__(self, backbone: str, backbone_hidden_size: int, nb_classes: int):\n",
        "        super(ReviewClassifier, self).__init__()\n",
        "        self.backbone = backbone\n",
        "        self.backbone_hidden_size = backbone_hidden_size\n",
        "        self.nb_classes = nb_classes\n",
        "\n",
        "        self.back_bone = AutoModel.from_pretrained(\n",
        "            self.backbone,\n",
        "            output_attentions=False,\n",
        "            output_hidden_states=False,\n",
        "        )\n",
        "        self.classifier = torch.nn.Linear(self.backbone_hidden_size, self.nb_classes)\n",
        "\n",
        "    def forward(\n",
        "        self, input_ids: torch.Tensor, attention_mask: torch.Tensor, labels: Optional[torch.Tensor] = None\n",
        "    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        back_bone_output = self.back_bone(input_ids, attention_mask=attention_mask)\n",
        "        hidden_states = back_bone_output[0]\n",
        "        pooled_output = hidden_states[:, 0]  # getting the [CLS] token\n",
        "\n",
        "        logits = self.classifier(pooled_output)\n",
        "        if labels is not None:\n",
        "            loss_fn = torch.nn.CrossEntropyLoss()\n",
        "            loss = loss_fn(\n",
        "                logits.view(-1, self.nb_classes),\n",
        "                labels.view(-1),\n",
        "            )\n",
        "            return loss, logits\n",
        "        return logits"
      ],
      "metadata": {
        "id": "m3mLhXkqeopZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ReviewClassifier(backbone=\"distilbert-base-uncased\", backbone_hidden_size=768, nb_classes=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HP58LrWUjFt4",
        "outputId": "8183cbb7-c906-4a8a-a71e-37edb2d046a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 🏓 Training Loop\n",
        "In this section we will define the training loop to trian our model. Note that these model are sensative wrt the hyperparameters and it usually takes a while to find the right hyperparameters. The default hyperparameters should work fine for our test case."
      ],
      "metadata": {
        "id": "RmOKVEhOemuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "import numpy as np\n",
        "\n",
        "print(f\"--> Device selected: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaqWfQRdmp_T",
        "outputId": "7b63771a-337b-4e2d-dbfa-224ee47161dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Device selected: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(\n",
        "    model: torch.nn.Module, training_data_loader: DataLoader, optimizer: torch.optim.Optimizer, logging_frequency: int\n",
        "):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    epoch_loss = 0\n",
        "    logging_loss = 0\n",
        "    for step, batch in enumerate(training_data_loader):\n",
        "        batch = {key: value.to(device) for key, value in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        logging_loss += loss.item()\n",
        "\n",
        "        if (step + 1) % logging_frequency == 0:\n",
        "            print(f\"Training loss @ step {step+1}: {logging_loss/logging_frequency}\")\n",
        "            logging_loss = 0\n",
        "\n",
        "    return epoch_loss / len(training_data_loader)\n",
        "\n",
        "\n",
        "def evaluate(model: torch.nn.Module, test_data_loader: DataLoader, nb_classes: int):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    eval_loss = 0\n",
        "    correct_predictions = {i: 0 for i in range(nb_classes)}\n",
        "    total_predictions = {i: 0 for i in range(nb_classes)}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(test_data_loader):\n",
        "            batch = {key: value.to(device) for key, value in batch.items()}\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs[0]\n",
        "            eval_loss += loss.item()\n",
        "\n",
        "            predictions = np.argmax(outputs[1].detach().cpu().numpy(), axis=1)\n",
        "            for target, prediction in zip(batch[\"labels\"].cpu().numpy(), predictions):\n",
        "                if target == prediction:\n",
        "                    correct_predictions[target] += 1\n",
        "                total_predictions[target] += 1\n",
        "\n",
        "    accuracy = (100.0 * sum(correct_predictions.values())) / sum(total_predictions.values())\n",
        "    return accuracy, eval_loss / len(test_data_loader)"
      ],
      "metadata": {
        "id": "hhXqq3SknGTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 🧑‍🍳 Setting hyperparameters for training { run: \"auto\" }\n",
        "nb_epoch = 3 #@param {type: \"slider\", min:1, max:10, step:1}\n",
        "batch_size = 64 #@param {type: \"integer\"}\n",
        "logging_frequency = 5 #@param {type: \"integer\"}\n",
        "learning_rate = 1e-5 #@param {type: \"number\"}\n",
        "\n",
        "train_loader = DataLoader(dataset_train, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
        "test_loader = DataLoader(dataset_test, batch_size=batch_size, shuffle=False, collate_fn=collate)\n",
        "\n",
        "# setting up the optimizer\n",
        "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "optimizer_grouped_parameters = [\n",
        "    {\n",
        "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "        \"weight_decay\": 0.0,\n",
        "    },\n",
        "    {\n",
        "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
        "        \"weight_decay\": 0.0,\n",
        "    },\n",
        "]\n",
        "\n",
        "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=learning_rate, eps=1e-8)\n"
      ],
      "metadata": {
        "id": "muLjnpSpvXOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)\n",
        "\n",
        "train_bar = tqdm(range(nb_epoch), desc=\"Epoch\")\n",
        "for e in train_bar:\n",
        "    train_loss = train_one_epoch(model, train_loader, optimizer, logging_frequency)\n",
        "    eval_acc, eval_loss  = evaluate(model, test_loader, 2)\n",
        "    print(f\"    Epoch: {e+1} Loss/Test: {eval_loss}, Loss/Test: {train_loss}, Acc/Test: {eval_acc}\")\n",
        "    train_bar.set_postfix({\"Loss/Train\": train_loss, \"Loss/Test\": eval_loss, \"Acc/Test\": eval_acc})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268,
          "referenced_widgets": [
            "48d33845788e40b7a5185f18b80bf0ae",
            "bdd3f08a72f74a47a0a4fa326ba98bba",
            "3dae6a5a03a44607a18909460d85b4d6",
            "7582bb5f909e45d78e248fc3c34a4535",
            "056b155d7aac48c09d6d97ea1ea3c6f7",
            "106716dec28142978e00f3f459a69214",
            "284dc766d41945828354e782e515319c",
            "b94e02d0a4d2493ba1206be86de1410f",
            "a4d95243125d4d8fa9ccc6db7896a8aa",
            "8b998d1186954872b81d097fdddd4336",
            "2be8a70acf324491aae37716eb6ccc2d"
          ]
        },
        "id": "RQhas_JxwVHS",
        "outputId": "ed466d10-f68f-4c7e-f2fe-4827ca8ee389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48d33845788e40b7a5185f18b80bf0ae",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss @ step 5: 0.6976208567619324\n",
            "Training loss @ step 10: 0.685474693775177\n",
            "Training loss @ step 15: 0.6706427097320556\n",
            "    Epoch: 1 Loss/Test: 0.6685808300971985, Loss/Test: 0.6822735592722893, Acc/Test: 48.5\n",
            "Training loss @ step 5: 0.6282077550888061\n",
            "Training loss @ step 10: 0.5967875123023987\n",
            "Training loss @ step 15: 0.5609706997871399\n",
            "    Epoch: 2 Loss/Test: 0.48369213938713074, Loss/Test: 0.5842777695506811, Acc/Test: 81.5\n",
            "Training loss @ step 5: 0.436927855014801\n",
            "Training loss @ step 10: 0.39156692028045653\n",
            "Training loss @ step 15: 0.3579081892967224\n",
            "    Epoch: 3 Loss/Test: 0.35278070345520973, Loss/Test: 0.39535488560795784, Acc/Test: 84.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🗃️ Exercises\n",
        "It is suggested that you have look over the `tokenizer` class and its functionalities before attempting the exercises."
      ],
      "metadata": {
        "id": "dyS-iVJJAskF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1️⃣ Predict with more context\n",
        "In the above training we only took advantage of the `title` of each review to predict its polarity.\n",
        "1. Investigate whether it would be useful to instead use the `content` of each review?\n",
        "2. Further investigate if it would be usefult to have both the `title` and `content` presented to model during training?"
      ],
      "metadata": {
        "id": "Zs4e1wmqBmGI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2️⃣ Frozen representations\n",
        "Modify the backbone so that we would only train the classifier layer, and the backbone stays frozen. How does the results compare to the unfrozen version?"
      ],
      "metadata": {
        "id": "5UdJVpMfDo1X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3️⃣ (Optional) Freeze then unfreeze\n",
        "It has empirically been shown that freezing the backbone for the first few steps of training and then unfreezing it produces better performing models. Modify the training code to have this option for training. "
      ],
      "metadata": {
        "id": "LBXdWeRSD_ez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4️⃣ (Optional) Build an emotion aware AI\n",
        "Lets now put everything we learned to the test by building an agent with some emotion detection abilities. Use the [emotion dataset](https://huggingface.co/datasets/emotion) to train an [ALBERT](https://huggingface.co/docs/transformers/model_doc/albert)-based model to detect the six basic emotions in our datasets. (anger, fear, joy, love, sadness, and surprise)"
      ],
      "metadata": {
        "id": "RWzdHXQeEq4y"
      }
    }
  ]
}