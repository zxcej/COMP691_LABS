{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zxcej/COMP691_LABS/blob/main/assignment2_starter_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü¶ú NN-Based Language Model\n",
        "In this excercise we will run a basic RNN based language model and answer some questions about the code. It is advised to use GPU to run the code. First run the code then answer the questions below that require modifying it."
      ],
      "metadata": {
        "id": "eNtvb_zpN4H7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTOJyYyujICY"
      },
      "source": [
        "#@title üßÆ Imports & Hyperparameter Setup\n",
        "#@markdown Feel free to experiment with the following hyperparameters at your\n",
        "#@markdown leasure. For the purpose of this assignment, leave the default values\n",
        "#@markdown and run the code with these suggested values.\n",
        "# Some part of the code was referenced from below.\n",
        "# https://github.com/pytorch/examples/tree/master/word_language_model \n",
        "# https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/02-intermediate/language_model\n",
        "\n",
        "! git clone https://github.com/yunjey/pytorch-tutorial/\n",
        "%cd pytorch-tutorial/tutorials/02-intermediate/language_model/\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "embed_size = 128 #@param {type:\"number\"}\n",
        "hidden_size = 1024 #@param {type:\"number\"}\n",
        "num_layers = 1 #@param {type:\"number\"}\n",
        "num_epochs = 5 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "batch_size = 20 #@param {type:\"number\"}\n",
        "seq_length = 30 #@param {type:\"number\"}\n",
        "learning_rate = 0.002 #@param {type:\"number\"}\n",
        "#@markdown Number of words to be sampled ‚¨áÔ∏è\n",
        "num_samples = 50 #@param {type:\"number\"}  \n",
        "\n",
        "print(f\"--> Device selected: {device}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzj73P_QeBEA"
      },
      "source": [
        "from data_utils import Dictionary, Corpus\n",
        "\n",
        "# Load \"Penn Treebank\" dataset\n",
        "corpus = Corpus()\n",
        "ids = corpus.get_data('data/train.txt', batch_size)\n",
        "vocab_size = len(corpus.dictionary)\n",
        "num_batches = ids.size(1) // seq_length\n",
        "\n",
        "print(f\"Vcoabulary size: {vocab_size}\")\n",
        "print(f\"Number of batches: {num_batches}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKzalmp8dndK"
      },
      "source": [
        "## ü§ñ Model Definition\n",
        "As you can see below, this model stacks `num_layers` many [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) units vertically to construct our basic RNN-based language model. The diagram below shows a pictorial representation of the model in its simplest form (i.e `num_layers`=1).\n",
        "![Pictorial Representation of The Model](https://upload.wikimedia.org/wikipedia/commons/6/63/Long_Short-Term_Memory.svg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZTjM5fQri35"
      },
      "source": [
        "# RNN based language model\n",
        "class RNNLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n",
        "        super(RNNLM, self).__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        \n",
        "    def forward(self, x, h):\n",
        "        # Embed word ids to vectors\n",
        "        x = self.embed(x)\n",
        "        \n",
        "        # Forward propagate LSTM\n",
        "        out, (h, c) = self.lstm(x, h)\n",
        "        \n",
        "        # Reshape output to (batch_size*sequence_length, hidden_size)\n",
        "        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n",
        "        \n",
        "        # Decode hidden states of all time steps\n",
        "        out = self.linear(out)\n",
        "        return out, (h, c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_fjTZ6wdpae"
      },
      "source": [
        "## üèì Training\n",
        "In this section we will train our model, this should take a couple of minutes! Be patient üòä"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsaIIUUHjQ5n"
      },
      "source": [
        "model = RNNLM(vocab_size, embed_size, hidden_size, num_layers).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Truncated backpropagation\n",
        "def detach(states):\n",
        "    return [state.detach() for state in states] \n",
        "\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    # Set initial hidden and cell states\n",
        "    states = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n",
        "              torch.zeros(num_layers, batch_size, hidden_size).to(device))\n",
        "    \n",
        "    for i in range(0, ids.size(1) - seq_length, seq_length):\n",
        "        # Get mini-batch inputs and targets\n",
        "        inputs = ids[:, i:i+seq_length].to(device)\n",
        "        targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        states = detach(states)\n",
        "        outputs, states = model(inputs, states)\n",
        "        loss = criterion(outputs, targets.reshape(-1))\n",
        "        \n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        step = (i+1) // seq_length\n",
        "        if step % 100 == 0:\n",
        "            print ('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n",
        "                   .format(epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü§î Questions"
      ],
      "metadata": {
        "id": "-Vy9OJMEXRJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1Ô∏è‚É£ Q2.1 Detaching or not? (10 points)\n",
        "The above code implements a version of truncated backpropagation through time. The implementation only requires the `detach()` function (lines 7-9 of the cell) defined above the loop and used once inside the training loop.\n",
        "* Explain the implementation (compared to not using truncated backprop through time).\n",
        "* What does the `detach()` call here achieve? Draw a computational graph. You may choose to answer this question outside the notebook.\n",
        "* When using using line 7-9 we will typically observe less GPU memory being used during training, explain why in your answer.\n"
      ],
      "metadata": {
        "id": "jhis12qSX-ce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîÆ Model Prediction\n",
        "Below we will use our model to generate text sequence!"
      ],
      "metadata": {
        "id": "lbyKZiTgahSv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxQ13QcIjPE9"
      },
      "source": [
        "# Sample from the model\n",
        "with torch.no_grad():\n",
        "    with open('sample.txt', 'w') as f:\n",
        "        # Set intial hidden ane cell states\n",
        "        state = (torch.zeros(num_layers, 1, hidden_size).to(device),\n",
        "                 torch.zeros(num_layers, 1, hidden_size).to(device))\n",
        "\n",
        "        # Select one word id randomly\n",
        "        prob = torch.ones(vocab_size)\n",
        "        input = torch.multinomial(prob, num_samples=1).unsqueeze(1).to(device)\n",
        "\n",
        "        for i in range(num_samples):\n",
        "            # Forward propagate RNN \n",
        "            output, state = model(input, state)\n",
        "\n",
        "            # Sample a word id\n",
        "            prob = output.exp()\n",
        "            word_id = torch.multinomial(prob, num_samples=1).item()\n",
        "\n",
        "            # Fill input with sampled word id for the next time step\n",
        "            input.fill_(word_id)\n",
        "\n",
        "            # File write\n",
        "            word = corpus.dictionary.idx2word[word_id]\n",
        "            word = '\\n' if word == '<eos>' else word + ' '\n",
        "            f.write(word)\n",
        "\n",
        "            if (i+1) % 100 == 0:\n",
        "                print('Sampled [{}/{}] words and save to {}'.format(i+1, num_samples, 'sample.txt'))\n",
        "! cat sample.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2Ô∏è‚É£ Q2.2 Sampling strategy (7 points)\n",
        "Consider the sampling procedure above. The current code samples a word:\n",
        "```python\n",
        "word_id = torch.multinomial(prob, num_samples=1).item()\n",
        "```\n",
        "in order to feed the model at each output step and feeding those to the next timestep. Copy below the above cell and modify this sampling startegy to use a greedy sampling which selects the highest probability word at each time step to feed as the next input."
      ],
      "metadata": {
        "id": "xXsUDt0tbAHM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BeO7LSWiyIZ"
      },
      "source": [
        "# Sample greedily from the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3Ô∏è‚É£ Q2.3 Embedding Distance (8 points)\n",
        "Our model has learned a specific set of word embeddings.\n",
        "* Write a function that takes in 2 words and prints the cosine distance between their embeddings using the word embeddings from the above models.\n",
        "* Use it to print the cosine distance of the word \"army\" and the word \"taxpayer\".\n",
        "\n",
        "*Refer to the sampling code for how to output the words corresponding to each index. To get the index you can use the function `corpus.dictionary.word2idx.`*\n"
      ],
      "metadata": {
        "id": "o8YV7laBe9er"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6w3JSY3d_6c"
      },
      "source": [
        "# Embedding distance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4Ô∏è‚É£ Q2.4 Teacher Forcing (Extra Credit 2 points)\n",
        "What is teacher forcing?\n",
        "> Teacher forcing works by using the actual or expected output from the training dataset at the current time step $y(t)$ as input in the next time step $X(t+1)$, rather than the output generated by the network.\n",
        "\n",
        "In the `üèì Training` code this is achieved, implicitly, when we pass the entire input sequence (`inputs = ids[:, i:i+seq_length].to(device)`) to the model at once.\n",
        "\n",
        "Copy below the `üèì Training` code and modify it to disable teacher forcing training. Compare the performance of this model, to original model, what can you conclude? (compare perplexity and convergence rate)"
      ],
      "metadata": {
        "id": "O44EBrsQdA4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training code with Teacher Forcing"
      ],
      "metadata": {
        "id": "qfgf5pJGfL-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5Ô∏è‚É£ Q2.5 Distance Comparison (+1 point)\n",
        "Repeat the work you did for `3Ô∏è‚É£ Q2.3 Embedding Distance` for the model in `4Ô∏è‚É£ Q2.4 Teacher Forcing` and compare the distances produced by these two models (i.e. with and without the teacher forcing), what can you conclude?"
      ],
      "metadata": {
        "id": "xH2TG4v3PBOu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EABSoOAGPAaS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}